{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set files provided in Task 6\n",
    "hygiene_text_path= \"./Hygiene/hygiene.dat\"\n",
    "hygiene_labels_path= \"./Hygiene/hygiene.dat.labels\"\n",
    "hygiene_additional_path= \"./Hygiene/hygiene.dat.additional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric, strip_short\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, stem_text\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from UtilWordEmbedding import MeanEmbeddingVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "SEED=26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprpcessing\n",
    "Preprocessing Steps:\n",
    "* tokenization and cleaning\n",
    "* stopword removal\n",
    "* stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and preprocess\n",
    "# https://radimrehurek.com/gensim/parsing/preprocessing.html\n",
    "FILTERS_LIST = [lambda x: x.lower(), # lowercase  \n",
    "                strip_tags, # remove tags\n",
    "                strip_punctuation, # replace punctuation characters with spaces\n",
    "                strip_multiple_whitespaces, # remove repeating whitespaces\n",
    "                # strip_numeric, # remove numbers\n",
    "                gensim.parsing.preprocessing.remove_stopwords, # remove stopwords\n",
    "                strip_short, # remove words less than minsize=3 characters long]\n",
    "                stem_text]\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, \n",
    "    \"\"\"\n",
    "    result_stemmed = []\n",
    "    for token in gensim.parsing.preprocessing.preprocess_string(text, FILTERS_LIST):\n",
    "        result_stemmed.append(WordNetLemmatizer().lemmatize(token))\n",
    "    return result_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13299/13299 [02:02<00:00, 108.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = []\n",
    "preprocessed_texts = []\n",
    "\n",
    "with open(hygiene_text_path) as f:\n",
    "    texts = f.readlines()\n",
    "    \n",
    "for _text in tqdm(texts):\n",
    "    result_stemmed = preprocess(_text)\n",
    "    preprocessed_texts.append(result_stemmed)\n",
    "    \n",
    "all_preprocessed_texts = [\" \".join(_text) for _text in preprocessed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13299 entries, 0 to 13298\n",
      "Data columns (total 8 columns):\n",
      "label                 13299 non-null object\n",
      "text                  13299 non-null object\n",
      "preprocessed_texts    13299 non-null object\n",
      "tokenized_texts       13299 non-null object\n",
      "cuisines_offered      13299 non-null object\n",
      "zipcode               13299 non-null object\n",
      "num_reviews           13299 non-null object\n",
      "avg_rating            13299 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 831.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>tokenized_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>[baguett, roll, excel, haven, tri, excit, doze...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>[live, street, betti, 160, sister, town, sprin...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>[worri, review, place, strongli, think, bad, n...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>[access, googl, street, view, like, medina, ya...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>[thing, like, place, homemad, guacamol, variet...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text                                 preprocessed_texts                                    tokenized_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0     1  The baguettes and rolls are excellent, and alt...  baguett roll excel haven tri excit dozen plu t...  [baguett, roll, excel, haven, tri, excit, doze...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1     1  I live up the street from Betty. &#160;When my...  live street betti 160 sister town spring break...  [live, street, betti, 160, sister, town, sprin...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2     1  I'm worried about how I will review this place...  worri review place strongli think bad night pl...  [worri, review, place, strongli, think, bad, n...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3     0  Why can't you access them on Google street vie...  access googl street view like medina yarrow po...  [access, googl, street, view, like, medina, ya...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4     0  Things to like about this place: homemade guac...  thing like place homemad guacamol varieti tast...  [thing, like, place, homemad, guacamol, variet...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 546\n",
    "\n",
    "# labels \n",
    "with open(hygiene_labels_path, 'r') as f:\n",
    "    labels = [l.rstrip() for l in f]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"label\":labels, \"text\": texts, \n",
    "                   \"preprocessed_texts\": all_preprocessed_texts,\n",
    "                   \"tokenized_texts\": preprocessed_texts})\n",
    "hygiene_additional = pd.read_csv(hygiene_additional_path,  \n",
    "                                 names=[\"cuisines_offered\", \"zipcode\", \"num_reviews\", \"avg_rating\"],\n",
    "                                 dtype={\"cuisines_offered\": str, \n",
    "                                        \"zipcode\": str,\n",
    "                                        \"num_reviews\": str})\n",
    "df = df.join(hygiene_additional)\n",
    "df['avg_rating'] = df['avg_rating'].apply(lambda x: str(int(round(x, 0))))\n",
    "\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 5) (546, 5) (546, 5) (546,)\n",
      "(12753, 5) (12753, 5) (12753, 5) (12753,)\n",
      "text                object\n",
      "cuisines_offered    object\n",
      "zipcode             object\n",
      "num_reviews         object\n",
      "avg_rating          object\n",
      "dtype: object preprocessed_texts    object\n",
      "cuisines_offered      object\n",
      "zipcode               object\n",
      "num_reviews           object\n",
      "avg_rating            object\n",
      "dtype: object tokenized_texts     object\n",
      "cuisines_offered    object\n",
      "zipcode             object\n",
      "num_reviews         object\n",
      "avg_rating          object\n",
      "dtype: object\n",
      "Wall time: 21.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = df[df[\"label\"] != \"[None]\"]\n",
    "test_df = df[df[\"label\"] == \"[None]\"]\n",
    "\n",
    "additional_feats = [\"cuisines_offered\", \"zipcode\", \"num_reviews\", \"avg_rating\"]\n",
    "\n",
    "train = train_df[[\"text\"] + additional_feats]\n",
    "train_preprocessed = train_df[[\"preprocessed_texts\"] + additional_feats]\n",
    "train_tokenized = train_df[[\"tokenized_texts\"] + additional_feats]\n",
    "train_labels = train_df[\"label\"].astype(int) # needed by sklearn\n",
    "\n",
    "test = test_df[[\"text\"] + additional_feats]\n",
    "test_preprocessed = test_df[[\"preprocessed_texts\"] + additional_feats]\n",
    "test_tokenized = test_df[[\"tokenized_texts\"] + additional_feats]\n",
    "test_labels = test_df[\"label\"]\n",
    "\n",
    "print(train.shape, train_preprocessed.shape, train_tokenized.shape, train_labels.shape)\n",
    "print(test.shape, test_preprocessed.shape, test_tokenized.shape, test_labels.shape)\n",
    "print(train.dtypes, train_preprocessed.dtypes, train_tokenized.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The baguettes and rolls are excellent, and alt...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I live up the street from Betty. &amp;#160;When my...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm worried about how I will review this place...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why can't you access them on Google street vie...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Things to like about this place: homemade guac...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  The baguettes and rolls are excellent, and alt...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  I live up the street from Betty. &#160;When my...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  I'm worried about how I will review this place...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  Why can't you access them on Google street vie...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  Things to like about this place: homemade guac...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baguett roll excel haven tri excit dozen plu t...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>live street betti 160 sister town spring break...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worri review place strongli think bad night pl...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>access googl street view like medina yarrow po...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thing like place homemad guacamol varieti tast...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  preprocessed_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  baguett roll excel haven tri excit dozen plu t...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  live street betti 160 sister town spring break...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  worri review place strongli think bad night pl...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  access googl street view like medina yarrow po...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  thing like place homemad guacamol varieti tast...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_texts</th>\n",
       "      <th>cuisines_offered</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[baguett, roll, excel, haven, tri, excit, doze...</td>\n",
       "      <td>['Vietnamese', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>98118</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[live, street, betti, 160, sister, town, sprin...</td>\n",
       "      <td>['American (New)', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[worri, review, place, strongli, think, bad, n...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98103</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[access, googl, street, view, like, medina, ya...</td>\n",
       "      <td>['Mexican', 'Tex-Mex', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[thing, like, place, homemad, guacamol, variet...</td>\n",
       "      <td>['Mexican', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tokenized_texts                             cuisines_offered zipcode num_reviews avg_rating\n",
       "0  [baguett, roll, excel, haven, tri, excit, doze...  ['Vietnamese', 'Sandwiches', 'Restaurants']   98118           4          4\n",
       "1  [live, street, betti, 160, sister, town, sprin...            ['American (New)', 'Restaurants']   98109          21          4\n",
       "2  [worri, review, place, strongli, think, bad, n...                   ['Mexican', 'Restaurants']   98103          14          3\n",
       "3  [access, googl, street, view, like, medina, ya...        ['Mexican', 'Tex-Mex', 'Restaurants']   98112          42          4\n",
       "4  [thing, like, place, homemad, guacamol, variet...                   ['Mexican', 'Restaurants']   98102          12          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.head())\n",
    "display(train_preprocessed.head())\n",
    "display(train_tokenized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applied Methods\n",
    "\n",
    "Models:  \n",
    "* Naive Bayes\n",
    "* SVM\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62650104 0.71815853 0.67175066 0.69420849 0.60238429]\n",
      "Average F1-Score: 0.66260\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "\n",
    "scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1_macro')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62857143 0.71559633 0.68965517 0.66666667 0.52747253]\n",
      "Average F1-Score: 0.64559\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'text')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "\n",
    "scores = cross_val_score(pipeline, train, train_labels, cv=5, scoring= 'f1')\n",
    "print(scores)\n",
    "print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(df['num_reviews'].value_counts()))\n",
    "#df['num_reviews'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(df['cuisines_offered'].value_counts()))\n",
    "#df['cuisines_offered'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(df['avg_rating'].value_counts()))\n",
    "print(len(df['zipcode'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_classifier(clf, X, y, vectorizer, text_col='text'):\n",
    "    pipeline = Pipeline([\n",
    "        ('union', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', vectorizer, text_col)],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "        ('clf', clf)\n",
    "    ], verbose=False)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring= 'f1_macro')\n",
    "    print(clf)\n",
    "    print(scores)\n",
    "    cv_score = np.average(scores)\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Support Vector Machine': svm.SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=SEED, n_estimators=500, n_jobs=-1),\n",
    "    #'Gradient Boosting': GradientBoostingClassifier()\n",
    "    'XGBoost': XGBClassifier(n_estimators=500, \n",
    "                            max_depth=5, \n",
    "                            learning_rate=0.2, \n",
    "                            objective='binary:logistic',\n",
    "                            scale_pos_weight=2,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=SEED)\n",
    "}\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500)\n",
    "bow = CountVectorizer(\n",
    "                stop_words=STOP_WORDS,\n",
    "                strip_accents='unicode',\n",
    "                min_df=15,\n",
    "                max_df=0.5,\n",
    "                ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BOW - No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.59880637 0.71797205 0.69080688 0.65714286 0.60625   ]\n",
      "Naive Bayes: 0.6541956293640896\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "[0.61117002 0.59430483 0.56227246 0.63291592 0.5459217 ]\n",
      "Support Vector Machine: 0.5893169873985258\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.56349206 0.57678153 0.59947037 0.62950257 0.62642684]\n",
      "Logistic Regression: 0.5991346769294893\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.6979782  0.60750145 0.55838073 0.63636364 0.65737803]\n",
      "Random Forest: 0.6315204104287556\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[0.62199313 0.60828157 0.56984774 0.63636364 0.63861004]\n",
      "XGBoost: 0.615019223339407\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train, train_labels, \n",
    "                               vectorizer=bow, text_col='text')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 BOW - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.60750145 0.73634185 0.68179188 0.63861004 0.60774818]\n",
      "Naive Bayes: 0.6543986809702782\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "[0.56468209 0.58632479 0.56463158 0.6082456  0.52574045]\n",
      "Support Vector Machine: 0.5699248992795691\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.55450864 0.53323904 0.60879993 0.65714286 0.6462069 ]\n",
      "Logistic Regression: 0.5999794725593286\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.6613695  0.6247608  0.58815209 0.61771562 0.6015444 ]\n",
      "Random Forest: 0.618708481096367\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[0.64187328 0.60879993 0.56984774 0.65501166 0.58329761]\n",
      "XGBoost: 0.6117660431959766\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train_preprocessed, train_labels, \n",
    "                               vectorizer=bow, text_col='preprocessed_texts')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TFIDF - No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.6447205  0.71815853 0.67175066 0.67567568 0.59173626]\n",
      "Naive Bayes: 0.6604083249868596\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "[0.68179188 0.76166667 0.68179188 0.60774818 0.55563506]\n",
      "Support Vector Machine: 0.6577267361102672\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.63588216 0.70909091 0.63527851 0.62950257 0.63636364]\n",
      "Logistic Regression: 0.6492235582335915\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.63624339 0.66338599 0.65454545 0.60990712 0.65501166]\n",
      "Random Forest: 0.6438187212245111\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[0.57277947 0.53447847 0.54303755 0.60017219 0.6015444 ]\n",
      "XGBoost: 0.570402415919\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train, train_labels, \n",
    "                               vectorizer=tfidf, text_col='text')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TFIDF - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[0.62650104 0.71815853 0.67175066 0.69420849 0.60238429]\n",
      "Naive Bayes: 0.662600601951647\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "[0.67272727 0.75289126 0.67272727 0.61771562 0.56696071]\n",
      "Support Vector Machine: 0.6566044248751868\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[0.65408805 0.72718254 0.67229394 0.61057692 0.61478904]\n",
      "Logistic Regression: 0.6557860988379179\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
      "                       warm_start=False)\n",
      "[0.60879993 0.63527851 0.59006211 0.66563467 0.64705882]\n",
      "Random Forest: 0.6293668117354396\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=26, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=2, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[0.58924571 0.50648056 0.56841139 0.59133127 0.58329761]\n",
      "XGBoost: 0.5677533055676471\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf in classifiers.items():\n",
    "    cv_score = test_classifier(clf, train_preprocessed, train_labels, \n",
    "                               vectorizer=tfidf, text_col='preprocessed_texts')\n",
    "    print('{}: {}'.format(clf_name, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_pred, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('sp2\\n')\n",
    "        for label in y_pred:\n",
    "            f.write(str(int(label)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ColumnTransformer(\n",
    "        [('cuisines_offered', CountVectorizer(min_df=10), 'cuisines_offered'),\n",
    "         ('zipcode', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['zipcode']),\n",
    "         ('num_reviews', CountVectorizer(max_df=7, token_pattern='\\d+'), 'num_reviews'),\n",
    "         ('avg_rating', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['avg_rating']),\n",
    "         ('text', TfidfVectorizer(\n",
    "                    stop_words='english',\n",
    "                    strip_accents='unicode',\n",
    "                    min_df=3,\n",
    "                    max_df=0.5,\n",
    "                    ngram_range=(1, 3),\n",
    "                    max_features=500), 'preprocessed_texts')],\n",
    "        remainder='passthrough',\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "], verbose=False)\n",
    "\n",
    "pipeline.fit(train_preprocessed, train_labels)\n",
    "y_pred = pipeline.predict(test_preprocessed)\n",
    "# score = metrics.f1_score(y_test, y_pred)\n",
    "# scores = cross_val_score(pipeline, train_preprocessed, train_labels, cv=5, scoring= 'f1_macro')\n",
    "# print(scores)\n",
    "# print(\"Average F1-Score: %0.5f\" % np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path ='./submissions/submission_task6.txt'\n",
    "create_submission(y_pred, submit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./submissions/submission_task6.txt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission completed successfully!\n"
     ]
    }
   ],
   "source": [
    "!python submit.py sp2 {submit_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
